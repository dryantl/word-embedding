{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Useful Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Embedder\n",
    "from gensim.models import FastText\n",
    "\n",
    "# Classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.grid_search import GridSearchCV as GS\n",
    "from sklearn.model_selection import validation_curve, learning_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures as Poly\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine Model's File Location\n",
    "\n",
    "version = \"version_5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_pipeline import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/{}/word_embedder_20_new.pickle\".format(version), \"rb\") as file:\n",
    "    word_embedder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<preprocessing_pipeline.preprocessing at 0x4658c38a58>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor=preprocessing(word_embedder.vector_size,word_embedder)\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.fasttext.FastText at 0x4658c38c50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data To Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_data_for_classification=pd.read_csv(\"data/big.csv\",header=None)\n",
    "large_data_for_classification.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hardware</td>\n",
       "      <td>KINGSTON+KVR1333D3N9</td>\n",
       "      <td>1510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>musik</td>\n",
       "      <td>power+amplifier+wisdom+</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>outwear-motor</td>\n",
       "      <td>jas%20hujan%20anak</td>\n",
       "      <td>391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>celana</td>\n",
       "      <td>Celana+bahan+formal</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>komputer</td>\n",
       "      <td>Preset+lightroom</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0                        1       2\n",
       "0       hardware     KINGSTON+KVR1333D3N9  1510.0\n",
       "1          musik  power+amplifier+wisdom+    62.0\n",
       "2  outwear-motor       jas%20hujan%20anak   391.0\n",
       "3         celana      Celana+bahan+formal   288.0\n",
       "4       komputer         Preset+lightroom     1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_data_for_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_category_mapper=pd.read_csv(\"category_mapping.csv\",index_col=0)\n",
    "\n",
    "category_mapper={}\n",
    "for i in raw_category_mapper.index:\n",
    "    category_mapper[raw_category_mapper[\"l2\"][i]]=raw_category_mapper[\"l1\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_category=[category_mapper[value] for value in large_data_for_classification[0]]\n",
    "large_data_for_classification[0]=new_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>komputer</td>\n",
       "      <td>KINGSTON+KVR1333D3N9</td>\n",
       "      <td>1510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hobi_dan_koleksi</td>\n",
       "      <td>power+amplifier+wisdom+</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>motor</td>\n",
       "      <td>jas%20hujan%20anak</td>\n",
       "      <td>391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fashion_wanita</td>\n",
       "      <td>Celana+bahan+formal</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>komputer</td>\n",
       "      <td>Preset+lightroom</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0                        1       2\n",
       "0          komputer     KINGSTON+KVR1333D3N9  1510.0\n",
       "1  hobi_dan_koleksi  power+amplifier+wisdom+    62.0\n",
       "2             motor       jas%20hujan%20anak   391.0\n",
       "3    fashion_wanita      Celana+bahan+formal   288.0\n",
       "4          komputer         Preset+lightroom     1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_data_for_classification.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING PUNCTUATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 396099/396099 [00:04<00:00, 87591.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING SENTENCE TO VECTOR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 396099/396099 [00:14<00:00, 27776.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE VECTOR TO PANDAS DATAFRAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 100/100 [00:16<00:00,  6.20it/s]\n"
     ]
    }
   ],
   "source": [
    "#preprocess product title to 100-dimensional vector\n",
    "#and preprocess category name to integer label\n",
    "large_embedded_data, large_label_encoder = preprocessor.preprocess_data(\n",
    "    large_data_for_classification[1],\n",
    "    large_data_for_classification[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.012349</td>\n",
       "      <td>-0.758770</td>\n",
       "      <td>-0.350302</td>\n",
       "      <td>0.166808</td>\n",
       "      <td>-0.751080</td>\n",
       "      <td>-0.460484</td>\n",
       "      <td>-0.092170</td>\n",
       "      <td>-0.565020</td>\n",
       "      <td>-0.285008</td>\n",
       "      <td>-0.081576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280283</td>\n",
       "      <td>-0.186133</td>\n",
       "      <td>-0.410584</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>-0.208072</td>\n",
       "      <td>0.005299</td>\n",
       "      <td>1.112637</td>\n",
       "      <td>0.402039</td>\n",
       "      <td>0.414983</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.795659</td>\n",
       "      <td>-2.895371</td>\n",
       "      <td>-1.744382</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>-0.306053</td>\n",
       "      <td>0.551503</td>\n",
       "      <td>-0.205609</td>\n",
       "      <td>-1.992900</td>\n",
       "      <td>-0.347923</td>\n",
       "      <td>0.280318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.375908</td>\n",
       "      <td>0.806441</td>\n",
       "      <td>0.564788</td>\n",
       "      <td>1.876183</td>\n",
       "      <td>1.148381</td>\n",
       "      <td>0.799000</td>\n",
       "      <td>0.305677</td>\n",
       "      <td>-0.643420</td>\n",
       "      <td>0.623411</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.960375</td>\n",
       "      <td>-0.504543</td>\n",
       "      <td>0.515864</td>\n",
       "      <td>0.896507</td>\n",
       "      <td>0.344629</td>\n",
       "      <td>0.505430</td>\n",
       "      <td>-1.599178</td>\n",
       "      <td>-1.249479</td>\n",
       "      <td>-1.578761</td>\n",
       "      <td>2.026936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.161255</td>\n",
       "      <td>0.004695</td>\n",
       "      <td>-1.989833</td>\n",
       "      <td>0.951701</td>\n",
       "      <td>0.752408</td>\n",
       "      <td>-1.261439</td>\n",
       "      <td>1.029763</td>\n",
       "      <td>0.189695</td>\n",
       "      <td>0.723305</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.369716</td>\n",
       "      <td>-0.911321</td>\n",
       "      <td>-0.494526</td>\n",
       "      <td>-0.126446</td>\n",
       "      <td>-0.021067</td>\n",
       "      <td>0.283687</td>\n",
       "      <td>0.012714</td>\n",
       "      <td>-0.269755</td>\n",
       "      <td>-0.109006</td>\n",
       "      <td>0.688226</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985987</td>\n",
       "      <td>0.006689</td>\n",
       "      <td>-1.561995</td>\n",
       "      <td>0.029477</td>\n",
       "      <td>0.272670</td>\n",
       "      <td>1.021256</td>\n",
       "      <td>1.340632</td>\n",
       "      <td>0.985763</td>\n",
       "      <td>0.906968</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.159122</td>\n",
       "      <td>-0.447368</td>\n",
       "      <td>-0.592817</td>\n",
       "      <td>0.131752</td>\n",
       "      <td>-1.350843</td>\n",
       "      <td>0.111322</td>\n",
       "      <td>-0.583496</td>\n",
       "      <td>-1.801566</td>\n",
       "      <td>0.540630</td>\n",
       "      <td>0.376193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004681</td>\n",
       "      <td>-0.056971</td>\n",
       "      <td>-0.586562</td>\n",
       "      <td>1.287600</td>\n",
       "      <td>0.441914</td>\n",
       "      <td>-0.338393</td>\n",
       "      <td>0.841190</td>\n",
       "      <td>1.215016</td>\n",
       "      <td>1.141698</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -1.012349 -0.758770 -0.350302  0.166808 -0.751080 -0.460484 -0.092170   \n",
       "1 -0.795659 -2.895371 -1.744382  0.395833 -0.306053  0.551503 -0.205609   \n",
       "2 -0.960375 -0.504543  0.515864  0.896507  0.344629  0.505430 -1.599178   \n",
       "3 -2.369716 -0.911321 -0.494526 -0.126446 -0.021067  0.283687  0.012714   \n",
       "4 -1.159122 -0.447368 -0.592817  0.131752 -1.350843  0.111322 -0.583496   \n",
       "\n",
       "          7         8         9   ...          91        92        93  \\\n",
       "0 -0.565020 -0.285008 -0.081576   ...   -0.280283 -0.186133 -0.410584   \n",
       "1 -1.992900 -0.347923  0.280318   ...   -0.375908  0.806441  0.564788   \n",
       "2 -1.249479 -1.578761  2.026936   ...   -0.161255  0.004695 -1.989833   \n",
       "3 -0.269755 -0.109006  0.688226   ...    0.985987  0.006689 -1.561995   \n",
       "4 -1.801566  0.540630  0.376193   ...    0.004681 -0.056971 -0.586562   \n",
       "\n",
       "         94        95        96        97        98        99  Labels  \n",
       "0  0.154500 -0.208072  0.005299  1.112637  0.402039  0.414983      10  \n",
       "1  1.876183  1.148381  0.799000  0.305677 -0.643420  0.623411       6  \n",
       "2  0.951701  0.752408 -1.261439  1.029763  0.189695  0.723305      12  \n",
       "3  0.029477  0.272670  1.021256  1.340632  0.985763  0.906968       3  \n",
       "4  1.287600  0.441914 -0.338393  0.841190  1.215016  1.141698      10  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_embedded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_embedded_data[\"sum\"]=large_embedded_data.drop([\"Labels\"],axis=1).sum(axis=1)\n",
    "large_embedded_data=large_embedded_data.loc[large_embedded_data[\"sum\"]!=0].drop(\"sum\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392825, 101)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_embedded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Product Title Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_product_title_data():\n",
    "    product_title_only=pd.read_fwf('data/products2m.txt',header=None)\n",
    "    product_title_only[\"Product Title\"]=product_title_only[0]\n",
    "    product_title_only=product_title_only[[\"Product Title\"]]\n",
    "    product_title_only.dropna(inplace=True,axis=0)\n",
    "\n",
    "    dummy_category=[0 for element in tqdm.tqdm(product_title_only[\"Product Title\"])]\n",
    "\n",
    "    embedded_product_title, dummy_encoder = preprocessor.preprocess_data(\n",
    "        product_title_only[\"Product Title\"],\n",
    "        dummy_category,\n",
    "    )\n",
    "    \n",
    "    embedded_product_title[\"sum\"]=embedded_product_title.drop([\"Labels\"],axis=1).sum(axis=1)\n",
    "    embedded_product_title=embedded_product_title.loc[embedded_product_title[\"sum\"]!=0].drop(\"sum\",axis=1)\n",
    "    embedded_product_title.drop(\"Labels\",axis=1,inplace=True)\n",
    "    \n",
    "    return embedded_product_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1938532/1938532 [00:01<00:00, 1839555.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING PUNCTUATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1938532/1938532 [00:41<00:00, 46917.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING SENTENCE TO VECTOR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████████████████████████████████████████████████████████            | 1617825/1938532 [02:48<00:33, 9580.41it/s]"
     ]
    }
   ],
   "source": [
    "product_title=read_product_title_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted,truth):\n",
    "    result=[int(value) for value in np.array(predicted)==np.array(truth)]\n",
    "    return sum(result)/len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_semi_supervised(dataset):\n",
    "    accepted_predictions=[]\n",
    "    for index in tqdm.tqdm(dataset.index):\n",
    "        predictions=[dataset[\"Prediction 1\"][index],\n",
    "                            dataset[\"Prediction 2\"][index],\n",
    "                            dataset[\"Prediction 3\"][index],\n",
    "                            dataset[\"Prediction 4\"][index],\n",
    "                            dataset[\"Prediction 5\"][index],\n",
    "                            dataset[\"Prediction 6\"][index],\n",
    "                            dataset[\"Prediction 7\"][index],\n",
    "                            dataset[\"Prediction 8\"][index],\n",
    "                            dataset[\"Prediction 9\"][index],\n",
    "                            dataset[\"Prediction 10\"][index]]\n",
    "        if(len(set(predictions))==1):\n",
    "            accepted_predictions.append(dataset[\"Prediction 1\"][index])\n",
    "        else:\n",
    "            accepted_predictions.append(-1)\n",
    "    dataset[\"Success\"]=accepted_predictions\n",
    "    \n",
    "    base_truth=dataset[\"Base Truth\"]\n",
    "    prediction=dataset[\"Success\"]\n",
    "    \n",
    "    count=0\n",
    "    total=0\n",
    "    for index,pred in tqdm.tqdm(enumerate(prediction)):\n",
    "        if(pred!=-1):\n",
    "            total+=1\n",
    "            if(pred==base_truth[index]):\n",
    "                count+=1\n",
    "    \n",
    "    return count, total, len(dataset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=large_embedded_data.copy()\n",
    "\n",
    "sampled_embedded_data=data.sample(n=len(data))\n",
    "\n",
    "nn_X_train,nn_X_test,nn_y_train,nn_y_test=train_test_split(sampled_embedded_data.drop(\"Labels\",axis=1),to_categorical(sampled_embedded_data[\"Labels\"]),test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1=nn_X_train[[*range(0,10)]]\n",
    "validation_features_1=nn_X_test[[*range(0,10)]]\n",
    "\n",
    "features_2=nn_X_train[[*range(10,20)]]\n",
    "validation_features_2=nn_X_test[[*range(10,20)]]\n",
    "\n",
    "features_3=nn_X_train[[*range(20,30)]]\n",
    "validation_features_3=nn_X_test[[*range(20,30)]]\n",
    "\n",
    "features_4=nn_X_train[[*range(30,40)]]\n",
    "validation_features_4=nn_X_test[[*range(30,40)]]\n",
    "\n",
    "features_5=nn_X_train[[*range(40,50)]]\n",
    "validation_features_5=nn_X_test[[*range(40,50)]]\n",
    "\n",
    "features_6=nn_X_train[[*range(50,60)]]\n",
    "validation_features_6=nn_X_test[[*range(50,60)]]\n",
    "\n",
    "features_7=nn_X_train[[*range(60,70)]]\n",
    "validation_features_7=nn_X_test[[*range(60,70)]]\n",
    "\n",
    "features_8=nn_X_train[[*range(70,80)]]\n",
    "validation_features_8=nn_X_test[[*range(70,80)]]\n",
    "\n",
    "features_9=nn_X_train[[*range(80,90)]]\n",
    "validation_features_9=nn_X_test[[*range(80,90)]]\n",
    "\n",
    "features_10=nn_X_train[[*range(90,100)]]\n",
    "validation_features_10=nn_X_test[[*range(90,100)]]\n",
    "\n",
    "labels=nn_y_train\n",
    "validation_labels=nn_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 353542 samples, validate on 39283 samples\n",
      "Epoch 1/5\n",
      " - 51s - loss: 1.9332 - acc: 0.4221 - val_loss: 1.8544 - val_acc: 0.4447\n",
      "Epoch 2/5\n",
      " - 49s - loss: 1.8097 - acc: 0.4600 - val_loss: 1.7942 - val_acc: 0.4635\n",
      "Epoch 3/5\n",
      " - 50s - loss: 1.7600 - acc: 0.4750 - val_loss: 1.7555 - val_acc: 0.4761\n",
      "Epoch 4/5\n",
      " - 49s - loss: 1.7263 - acc: 0.4856 - val_loss: 1.7306 - val_acc: 0.4850\n",
      "Epoch 5/5\n",
      " - 49s - loss: 1.7006 - acc: 0.4932 - val_loss: 1.7112 - val_acc: 0.4907\n"
     ]
    }
   ],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(750, input_shape=(10,), activation='relu'))\n",
    "model_1.add(Dense(500, activation='relu'))\n",
    "model_1.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model_1.compile(optimizer=\"Adagrad\", loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_1.fit(features_1, labels, epochs=5, batch_size=100, validation_data=(validation_features_1,validation_labels), shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 353542 samples, validate on 39283 samples\n",
      "Epoch 1/5\n",
      " - 50s - loss: 1.9047 - acc: 0.4257 - val_loss: 1.8371 - val_acc: 0.4513\n",
      "Epoch 2/5\n",
      " - 50s - loss: 1.7941 - acc: 0.4649 - val_loss: 1.7817 - val_acc: 0.4706\n",
      "Epoch 3/5\n",
      " - 50s - loss: 1.7450 - acc: 0.4821 - val_loss: 1.7407 - val_acc: 0.4884\n",
      "Epoch 4/5\n",
      " - 50s - loss: 1.7115 - acc: 0.4935 - val_loss: 1.7128 - val_acc: 0.4950\n",
      "Epoch 5/5\n",
      " - 50s - loss: 1.6854 - acc: 0.5018 - val_loss: 1.6913 - val_acc: 0.5018\n"
     ]
    }
   ],
   "source": [
    "model_2 = Sequential()\n",
    "model_2.add(Dense(750, input_shape=(10,), activation='relu'))\n",
    "model_2.add(Dense(500, activation='relu'))\n",
    "model_2.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model_2.compile(optimizer=\"Adagrad\", loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_2.fit(features_2, labels, epochs=5, batch_size=100, validation_data=(validation_features_2,validation_labels), shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 353542 samples, validate on 39283 samples\n",
      "Epoch 1/5\n",
      " - 51s - loss: 1.8915 - acc: 0.4258 - val_loss: 1.8135 - val_acc: 0.4528\n",
      "Epoch 2/5\n",
      " - 50s - loss: 1.7806 - acc: 0.4637 - val_loss: 1.7582 - val_acc: 0.4744\n",
      "Epoch 3/5\n",
      " - 50s - loss: 1.7350 - acc: 0.4792 - val_loss: 1.7306 - val_acc: 0.4840\n",
      "Epoch 4/5\n",
      " - 50s - loss: 1.7037 - acc: 0.4893 - val_loss: 1.7046 - val_acc: 0.4957\n",
      "Epoch 5/5\n",
      " - 50s - loss: 1.6792 - acc: 0.4977 - val_loss: 1.6845 - val_acc: 0.4994\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Dense(750, input_shape=(10,), activation='relu'))\n",
    "model_3.add(Dense(500, activation='relu'))\n",
    "model_3.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model_3.compile(optimizer=\"Adagrad\", loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_3.fit(features_3, labels, epochs=5, batch_size=100, validation_data=(validation_features_3,validation_labels), shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 353542 samples, validate on 39283 samples\n",
      "Epoch 1/5\n",
      " - 51s - loss: 1.9237 - acc: 0.4266 - val_loss: 1.8495 - val_acc: 0.4541\n",
      "Epoch 2/5\n",
      " - 50s - loss: 1.8103 - acc: 0.4643 - val_loss: 1.7899 - val_acc: 0.4737\n",
      "Epoch 3/5\n",
      " - 50s - loss: 1.7612 - acc: 0.4809 - val_loss: 1.7554 - val_acc: 0.4860\n",
      "Epoch 4/5\n",
      " - 50s - loss: 1.7276 - acc: 0.4917 - val_loss: 1.7270 - val_acc: 0.4939\n",
      "Epoch 5/5\n",
      " - 50s - loss: 1.7017 - acc: 0.4992 - val_loss: 1.7058 - val_acc: 0.4996\n"
     ]
    }
   ],
   "source": [
    "model_4 = Sequential()\n",
    "model_4.add(Dense(750, input_shape=(10,), activation='relu'))\n",
    "model_4.add(Dense(500, activation='relu'))\n",
    "model_4.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model_4.compile(optimizer=\"Adagrad\", loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_4.fit(features_4, labels, epochs=5, batch_size=100, validation_data=(validation_features_4,validation_labels), shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 353542 samples, validate on 39283 samples\n",
      "Epoch 1/5\n",
      " - 51s - loss: 1.8235 - acc: 0.4499 - val_loss: 1.7464 - val_acc: 0.4753\n",
      "Epoch 2/5\n",
      " - 50s - loss: 1.7171 - acc: 0.4854 - val_loss: 1.6918 - val_acc: 0.4933\n",
      "Epoch 3/5\n",
      " - 50s - loss: 1.6724 - acc: 0.4993 - val_loss: 1.6614 - val_acc: 0.5037\n",
      "Epoch 4/5\n",
      " - 50s - loss: 1.6416 - acc: 0.5094 - val_loss: 1.6346 - val_acc: 0.5121\n",
      "Epoch 5/5\n",
      " - 50s - loss: 1.6179 - acc: 0.5176 - val_loss: 1.6179 - val_acc: 0.5192\n"
     ]
    }
   ],
   "source": [
    "model_5 = Sequential()\n",
    "model_5.add(Dense(750, input_shape=(10,), activation='relu'))\n",
    "model_5.add(Dense(500, activation='relu'))\n",
    "model_5.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model_5.compile(optimizer=\"Adagrad\", loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_5.fit(features_5, labels, epochs=5, batch_size=100, validation_data=(validation_features_5,validation_labels), shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 353542 samples, validate on 39283 samples\n",
      "Epoch 1/5\n",
      " - 51s - loss: 1.8481 - acc: 0.4425 - val_loss: 1.7844 - val_acc: 0.4658\n",
      "Epoch 2/5\n",
      " - 50s - loss: 1.7432 - acc: 0.4773 - val_loss: 1.7300 - val_acc: 0.4865\n",
      "Epoch 3/5\n",
      " - 50s - loss: 1.6973 - acc: 0.4920 - val_loss: 1.6994 - val_acc: 0.4972\n",
      "Epoch 4/5\n",
      " - 50s - loss: 1.6655 - acc: 0.5030 - val_loss: 1.6723 - val_acc: 0.5017\n",
      "Epoch 5/5\n",
      " - 50s - loss: 1.6406 - acc: 0.5111 - val_loss: 1.6534 - val_acc: 0.5119\n"
     ]
    }
   ],
   "source": [
    "model_6 = Sequential()\n",
    "model_6.add(Dense(750, input_shape=(10,), activation='relu'))\n",
    "model_6.add(Dense(500, activation='relu'))\n",
    "model_6.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model_6.compile(optimizer=\"Adagrad\", loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_6.fit(features_6, labels, epochs=5, batch_size=100, validation_data=(validation_features_6,validation_labels), shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 353542 samples, validate on 39283 samples\n",
      "Epoch 1/5\n",
      " - 50s - loss: 1.8907 - acc: 0.4294 - val_loss: 1.8207 - val_acc: 0.4538\n",
      "Epoch 2/5\n",
      " - 50s - loss: 1.7809 - acc: 0.4646 - val_loss: 1.7652 - val_acc: 0.4737\n",
      "Epoch 3/5\n",
      " - 50s - loss: 1.7352 - acc: 0.4788 - val_loss: 1.7293 - val_acc: 0.4845\n",
      "Epoch 4/5\n",
      " - 49s - loss: 1.7042 - acc: 0.4893 - val_loss: 1.7055 - val_acc: 0.4948\n",
      "Epoch 5/5\n",
      " - 50s - loss: 1.6798 - acc: 0.4971 - val_loss: 1.6854 - val_acc: 0.4991\n"
     ]
    }
   ],
   "source": [
    "model_7 = Sequential()\n",
    "model_7.add(Dense(750, input_shape=(10,), activation='relu'))\n",
    "model_7.add(Dense(500, activation='relu'))\n",
    "model_7.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model_7.compile(optimizer=\"Adagrad\", loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_7.fit(features_7, labels, epochs=5, batch_size=100, validation_data=(validation_features_7,validation_labels), shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 353542 samples, validate on 39283 samples\n",
      "Epoch 1/5\n",
      " - 51s - loss: 1.9596 - acc: 0.4086 - val_loss: 1.8781 - val_acc: 0.4370\n",
      "Epoch 2/5\n",
      " - 50s - loss: 1.8423 - acc: 0.4474 - val_loss: 1.8173 - val_acc: 0.4590\n",
      "Epoch 3/5\n",
      " - 50s - loss: 1.7932 - acc: 0.4646 - val_loss: 1.7824 - val_acc: 0.4712\n",
      "Epoch 4/5\n",
      " - 50s - loss: 1.7600 - acc: 0.4758 - val_loss: 1.7563 - val_acc: 0.4773\n",
      "Epoch 5/5\n",
      " - 50s - loss: 1.7343 - acc: 0.4844 - val_loss: 1.7356 - val_acc: 0.4849\n"
     ]
    }
   ],
   "source": [
    "model_8 = Sequential()\n",
    "model_8.add(Dense(750, input_shape=(10,), activation='relu'))\n",
    "model_8.add(Dense(500, activation='relu'))\n",
    "model_8.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model_8.compile(optimizer=\"Adagrad\", loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_8.fit(features_8, labels, epochs=5, batch_size=100, validation_data=(validation_features_8,validation_labels), shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 353542 samples, validate on 39283 samples\n",
      "Epoch 1/5\n",
      " - 51s - loss: 1.8910 - acc: 0.4277 - val_loss: 1.8146 - val_acc: 0.4535\n",
      "Epoch 2/5\n",
      " - 50s - loss: 1.7804 - acc: 0.4638 - val_loss: 1.7574 - val_acc: 0.4738\n",
      "Epoch 3/5\n",
      " - 50s - loss: 1.7331 - acc: 0.4792 - val_loss: 1.7247 - val_acc: 0.4831\n",
      "Epoch 4/5\n",
      " - 50s - loss: 1.7004 - acc: 0.4894 - val_loss: 1.6994 - val_acc: 0.4915\n",
      "Epoch 5/5\n",
      " - 50s - loss: 1.6752 - acc: 0.4975 - val_loss: 1.6783 - val_acc: 0.5005\n"
     ]
    }
   ],
   "source": [
    "model_9 = Sequential()\n",
    "model_9.add(Dense(750, input_shape=(10,), activation='relu'))\n",
    "model_9.add(Dense(500, activation='relu'))\n",
    "model_9.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model_9.compile(optimizer=\"Adagrad\", loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_9.fit(features_9, labels, epochs=5, batch_size=100, validation_data=(validation_features_9,validation_labels), shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 353542 samples, validate on 39283 samples\n",
      "Epoch 1/5\n",
      " - 52s - loss: 1.9392 - acc: 0.4109 - val_loss: 1.8803 - val_acc: 0.4298\n",
      "Epoch 2/5\n",
      " - 51s - loss: 1.8243 - acc: 0.4502 - val_loss: 1.8158 - val_acc: 0.4549\n",
      "Epoch 3/5\n",
      " - 51s - loss: 1.7757 - acc: 0.4666 - val_loss: 1.7785 - val_acc: 0.4685\n",
      "Epoch 4/5\n",
      " - 51s - loss: 1.7428 - acc: 0.4773 - val_loss: 1.7505 - val_acc: 0.4736\n",
      "Epoch 5/5\n",
      " - 51s - loss: 1.7173 - acc: 0.4858 - val_loss: 1.7325 - val_acc: 0.4800\n"
     ]
    }
   ],
   "source": [
    "model_10 = Sequential()\n",
    "model_10.add(Dense(750, input_shape=(10,), activation='relu'))\n",
    "model_10.add(Dense(500, activation='relu'))\n",
    "model_10.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model_10.compile(optimizer=\"Adagrad\", loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "history = model_10.fit(features_10, labels, epochs=5, batch_size=100, validation_data=(validation_features_10,validation_labels), shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 39283/39283 [00:07<00:00, 5606.54it/s]\n",
      "39283it [00:00, 306083.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6531, 6856, 39283)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_semi_supervised(semi_supervised_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_classification_report(dataset):\n",
    "    accepted_predictions=[]\n",
    "    for index in tqdm.tqdm(dataset.index):\n",
    "        predictions=[dataset[\"Prediction 1\"][index],\n",
    "                            dataset[\"Prediction 2\"][index],\n",
    "                            dataset[\"Prediction 3\"][index],\n",
    "                            dataset[\"Prediction 4\"][index],\n",
    "                            dataset[\"Prediction 5\"][index],\n",
    "                            dataset[\"Prediction 6\"][index],\n",
    "                            dataset[\"Prediction 7\"][index],\n",
    "                            dataset[\"Prediction 8\"][index],\n",
    "                            dataset[\"Prediction 9\"][index],\n",
    "                            dataset[\"Prediction 10\"][index]]\n",
    "        if(len(set(predictions))==1):\n",
    "            accepted_predictions.append(dataset[\"Prediction 1\"][index])\n",
    "        else:\n",
    "            accepted_predictions.append(-1)\n",
    "    dataset[\"Success\"]=accepted_predictions\n",
    "    \n",
    "    base_truth=dataset[\"Base Truth\"]\n",
    "    prediction=dataset[\"Success\"]\n",
    "    \n",
    "    taken_base_truth=[]\n",
    "    taken_prediction=[]\n",
    "    for index,pred in tqdm.tqdm(enumerate(prediction)):\n",
    "        if(pred!=-1):\n",
    "            taken_base_truth.append(base_truth[index])\n",
    "            taken_prediction.append(pred)\n",
    "            \n",
    "    \n",
    "    print(classification_report(taken_base_truth,taken_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_dataset(model,features):\n",
    "    semi_supervised_prediction=pd.DataFrame()\n",
    "    print(\"PREDICTION 1\")\n",
    "    semi_supervised_prediction[\"Prediction 1\"]=[np.argmax(value) for value in tqdm.tqdm(model[0].predict(features[[*range(0,10)]]))]\n",
    "    print(\"PREDICTION 2\")\n",
    "    semi_supervised_prediction[\"Prediction 2\"]=[np.argmax(value) for value in tqdm.tqdm(model[1].predict(features[[*range(10,20)]]))]\n",
    "    print(\"PREDICTION 3\")\n",
    "    semi_supervised_prediction[\"Prediction 3\"]=[np.argmax(value) for value in tqdm.tqdm(model[2].predict(features[[*range(20,30)]]))]\n",
    "    print(\"PREDICTION 4\")\n",
    "    semi_supervised_prediction[\"Prediction 4\"]=[np.argmax(value) for value in tqdm.tqdm(model[3].predict(features[[*range(30,40)]]))]\n",
    "    print(\"PREDICTION 5\")\n",
    "    semi_supervised_prediction[\"Prediction 5\"]=[np.argmax(value) for value in tqdm.tqdm(model[4].predict(features[[*range(40,50)]]))]\n",
    "    print(\"PREDICTION 6\")\n",
    "    semi_supervised_prediction[\"Prediction 6\"]=[np.argmax(value) for value in tqdm.tqdm(model[5].predict(features[[*range(50,60)]]))]\n",
    "    print(\"PREDICTION 7\")\n",
    "    semi_supervised_prediction[\"Prediction 7\"]=[np.argmax(value) for value in tqdm.tqdm(model[6].predict(features[[*range(60,70)]]))]\n",
    "    print(\"PREDICTION 8\")\n",
    "    semi_supervised_prediction[\"Prediction 8\"]=[np.argmax(value) for value in tqdm.tqdm(model[7].predict(features[[*range(70,80)]]))]\n",
    "    print(\"PREDICTION 9\")\n",
    "    semi_supervised_prediction[\"Prediction 9\"]=[np.argmax(value) for value in tqdm.tqdm(model[8].predict(features[[*range(80,90)]]))]\n",
    "    print(\"PREDICTION 10\")\n",
    "    semi_supervised_prediction[\"Prediction 10\"]=[np.argmax(value) for value in tqdm.tqdm(model[9].predict(features[[*range(90,100)]]))]\n",
    "    \n",
    "    result_dataset=features.copy()\n",
    "    \n",
    "    accepted_predictions=[]\n",
    "    \n",
    "    print(\"ENSEMBLING\")\n",
    "    for index in tqdm.tqdm(semi_supervised_prediction.index):\n",
    "        predictions=[semi_supervised_prediction[\"Prediction {}\".format(i)][index] for i in range(1,10+1)]\n",
    "        \n",
    "        if(len(set(predictions))==1):\n",
    "            accepted_predictions.append(semi_supervised_prediction[\"Prediction 1\"][index])\n",
    "        else:\n",
    "            accepted_predictions.append(-1)\n",
    "            \n",
    "    result_dataset[\"Labels\"]=accepted_predictions\n",
    "    \n",
    "    result_dataset=result_dataset.loc[result_dataset[\"Labels\"]!=-1]\n",
    "    \n",
    "    return result_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[model_1,model_2,model_3,model_4,model_5,model_6,model_7,model_8,model_9,model_10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_dataset = semi_supervised_dataset(product_,embedded_product_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_y_truth=[np.argmax(value) for value in validation_labels]\n",
    "nn_y_pred=[np.argmax(value) for value in model_1.predict(validation_features_1)]\n",
    "print(\"Validation Accuracy : {}\".format(accuracy(nn_y_pred,nn_y_truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.save(\"level1_semsup_1.h5\")\n",
    "model_2.save(\"level1_semsup_2.h5\")\n",
    "model_3.save(\"level1_semsup_3.h5\")\n",
    "model_4.save(\"level1_semsup_4.h5\")\n",
    "model_5.save(\"level1_semsup_5.h5\")\n",
    "model_6.save(\"level1_semsup_6.h5\")\n",
    "model_7.save(\"level1_semsup_7.h5\")\n",
    "model_8.save(\"level1_semsup_8.h5\")\n",
    "model_9.save(\"level1_semsup_9.h5\")\n",
    "model_10.save(\"level1_semsup_10.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
