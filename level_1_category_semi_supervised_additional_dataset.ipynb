{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Useful Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Embedder\n",
    "from gensim.models import FastText\n",
    "\n",
    "# Classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.grid_search import GridSearchCV as GS\n",
    "from sklearn.model_selection import validation_curve, learning_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures as Poly\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine Model's File Location\n",
    "\n",
    "version = \"version_5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_pipeline import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor=preprocessing(word_embedder.vector_size,word_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/{}/word_embedder_20_new.pickle\".format(version), \"rb\") as file:\n",
    "    word_embedder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = load_model(\"model/{}/level1_semsup_1.h5\".format(version))\n",
    "model_2 = load_model(\"model/{}/level1_semsup_2.h5\".format(version))\n",
    "model_3 = load_model(\"model/{}/level1_semsup_3.h5\".format(version))\n",
    "model_4 = load_model(\"model/{}/level1_semsup_4.h5\".format(version))\n",
    "model_5 = load_model(\"model/{}/level1_semsup_5.h5\".format(version))\n",
    "model_6 = load_model(\"model/{}/level1_semsup_6.h5\".format(version))\n",
    "model_7 = load_model(\"model/{}/level1_semsup_7.h5\".format(version))\n",
    "model_8 = load_model(\"model/{}/level1_semsup_8.h5\".format(version))\n",
    "model_9 = load_model(\"model/{}/level1_semsup_9.h5\".format(version))\n",
    "model_10 = load_model(\"model/{}/level1_semsup_10.h5\".format(version))\n",
    "\n",
    "models=[model_1,model_2,model_3,model_4,model_5,model_6,model_7,model_8,model_9,model_10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data To Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_classification_data():\n",
    "    large_data_for_classification=pd.read_csv(\"data/big.csv\",header=None)\n",
    "    large_data_for_classification.dropna(axis=0,inplace=True)\n",
    "    raw_category_mapper=pd.read_csv(\"category_mapping.csv\",index_col=0)\n",
    "\n",
    "    category_mapper={}\n",
    "    for i in raw_category_mapper.index:\n",
    "        category_mapper[raw_category_mapper[\"l2\"][i]]=raw_category_mapper[\"l1\"][i]\n",
    "   \n",
    "    new_category=[category_mapper[value] for value in large_data_for_classification[0]]\n",
    "    large_data_for_classification[0]=new_category\n",
    "    \n",
    "    large_embedded_data, large_label_encoder = preprocessor.preprocess_data(\n",
    "        large_data_for_classification[1],\n",
    "        large_data_for_classification[0],\n",
    "    )\n",
    "    \n",
    "    large_embedded_data[\"sum\"]=large_embedded_data.drop([\"Labels\"],axis=1).sum(axis=1)\n",
    "    large_embedded_data=large_embedded_data.loc[large_embedded_data[\"sum\"]!=0].drop(\"sum\",axis=1)\n",
    "    \n",
    "    return large_embedded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_product_title_data():\n",
    "    product_title_only=pd.read_fwf('data/products2m.txt',header=None)\n",
    "    product_title_only[\"Product Title\"]=product_title_only[0]\n",
    "    product_title_only=product_title_only[[\"Product Title\"]]\n",
    "    product_title_only.dropna(inplace=True,axis=0)\n",
    "    \n",
    "    return (product_title_only[:300000],\n",
    "            product_title_only[300000:600000],\n",
    "            product_title_only[600000:900000],\n",
    "            product_title_only[900000:1200000],\n",
    "            product_title_only[1200000:1500000],\n",
    "            product_title_only[1500000:1800000],\n",
    "            product_title_only[1800000:])\n",
    "    \n",
    "    return product_title_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING PUNCTUATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 396099/396099 [00:05<00:00, 79183.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING SENTENCE TO VECTOR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 396099/396099 [00:15<00:00, 25078.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE VECTOR TO PANDAS DATAFRAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 100/100 [00:15<00:00,  6.37it/s]\n"
     ]
    }
   ],
   "source": [
    "large_embedded_data=read_classification_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_titles=read_product_title_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted,truth):\n",
    "    result=[int(value) for value in np.array(predicted)==np.array(truth)]\n",
    "    return sum(result)/len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_semi_supervised(dataset):\n",
    "    accepted_predictions=[]\n",
    "    for index in tqdm.tqdm(dataset.index):\n",
    "        predictions=[dataset[\"Prediction 1\"][index],\n",
    "                            dataset[\"Prediction 2\"][index],\n",
    "                            dataset[\"Prediction 3\"][index],\n",
    "                            dataset[\"Prediction 4\"][index],\n",
    "                            dataset[\"Prediction 5\"][index],\n",
    "                            dataset[\"Prediction 6\"][index],\n",
    "                            dataset[\"Prediction 7\"][index],\n",
    "                            dataset[\"Prediction 8\"][index],\n",
    "                            dataset[\"Prediction 9\"][index],\n",
    "                            dataset[\"Prediction 10\"][index]]\n",
    "        if(len(set(predictions))==1):\n",
    "            accepted_predictions.append(dataset[\"Prediction 1\"][index])\n",
    "        else:\n",
    "            accepted_predictions.append(-1)\n",
    "    dataset[\"Success\"]=accepted_predictions\n",
    "    \n",
    "    base_truth=dataset[\"Base Truth\"]\n",
    "    prediction=dataset[\"Success\"]\n",
    "    \n",
    "    count=0\n",
    "    total=0\n",
    "    for index,pred in tqdm.tqdm(enumerate(prediction)):\n",
    "        if(pred!=-1):\n",
    "            total+=1\n",
    "            if(pred==base_truth[index]):\n",
    "                count+=1\n",
    "    \n",
    "    return count, total, len(dataset)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_classification_report(dataset):\n",
    "    accepted_predictions=[]\n",
    "    for index in tqdm.tqdm(dataset.index):\n",
    "        predictions=[dataset[\"Prediction 1\"][index],\n",
    "                            dataset[\"Prediction 2\"][index],\n",
    "                            dataset[\"Prediction 3\"][index],\n",
    "                            dataset[\"Prediction 4\"][index],\n",
    "                            dataset[\"Prediction 5\"][index],\n",
    "                            dataset[\"Prediction 6\"][index],\n",
    "                            dataset[\"Prediction 7\"][index],\n",
    "                            dataset[\"Prediction 8\"][index],\n",
    "                            dataset[\"Prediction 9\"][index],\n",
    "                            dataset[\"Prediction 10\"][index]]\n",
    "        if(len(set(predictions))==1):\n",
    "            accepted_predictions.append(dataset[\"Prediction 1\"][index])\n",
    "        else:\n",
    "            accepted_predictions.append(-1)\n",
    "    dataset[\"Success\"]=accepted_predictions\n",
    "    \n",
    "    base_truth=dataset[\"Base Truth\"]\n",
    "    prediction=dataset[\"Success\"]\n",
    "    \n",
    "    taken_base_truth=[]\n",
    "    taken_prediction=[]\n",
    "    for index,pred in tqdm.tqdm(enumerate(prediction)):\n",
    "        if(pred!=-1):\n",
    "            taken_base_truth.append(base_truth[index])\n",
    "            taken_prediction.append(pred)\n",
    "            \n",
    "    \n",
    "    print(classification_report(taken_base_truth,taken_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_dataset(model,features):\n",
    "    \n",
    "    dummy_category=[0 for element in tqdm.tqdm(features[\"Product Title\"])]\n",
    "\n",
    "    embedded_product_title, dummy_encoder = preprocessor.preprocess_data(\n",
    "        features[\"Product Title\"],\n",
    "        dummy_category,\n",
    "    )\n",
    "    \n",
    "    embedded_product_title[\"sum\"]=embedded_product_title.drop([\"Labels\"],axis=1).sum(axis=1)\n",
    "    embedded_product_title=embedded_product_title.loc[embedded_product_title[\"sum\"]!=0].drop(\"sum\",axis=1)\n",
    "    embedded_product_title.drop(\"Labels\",axis=1,inplace=True)\n",
    "    \n",
    "    features=embedded_product_title\n",
    "    \n",
    "    semi_supervised_prediction=pd.DataFrame()\n",
    "    print(\"PREDICTION 1\")\n",
    "    semi_supervised_prediction[\"Prediction 1\"]=[np.argmax(value) for value in model[0].predict(features[[*range(0,10)]])]\n",
    "    print(\"PREDICTION 2\")\n",
    "    semi_supervised_prediction[\"Prediction 2\"]=[np.argmax(value) for value in model[1].predict(features[[*range(10,20)]])]\n",
    "    print(\"PREDICTION 3\")\n",
    "    semi_supervised_prediction[\"Prediction 3\"]=[np.argmax(value) for value in model[2].predict(features[[*range(20,30)]])]\n",
    "    print(\"PREDICTION 4\")\n",
    "    semi_supervised_prediction[\"Prediction 4\"]=[np.argmax(value) for value in model[3].predict(features[[*range(30,40)]])]\n",
    "    print(\"PREDICTION 5\")\n",
    "    semi_supervised_prediction[\"Prediction 5\"]=[np.argmax(value) for value in model[4].predict(features[[*range(40,50)]])]\n",
    "    print(\"PREDICTION 6\")\n",
    "    semi_supervised_prediction[\"Prediction 6\"]=[np.argmax(value) for value in model[5].predict(features[[*range(50,60)]])]\n",
    "    print(\"PREDICTION 7\")\n",
    "    semi_supervised_prediction[\"Prediction 7\"]=[np.argmax(value) for value in model[6].predict(features[[*range(60,70)]])]\n",
    "    print(\"PREDICTION 8\")\n",
    "    semi_supervised_prediction[\"Prediction 8\"]=[np.argmax(value) for value in model[7].predict(features[[*range(70,80)]])]\n",
    "    print(\"PREDICTION 9\")\n",
    "    semi_supervised_prediction[\"Prediction 9\"]=[np.argmax(value) for value in model[8].predict(features[[*range(80,90)]])]\n",
    "    print(\"PREDICTION 10\")\n",
    "    semi_supervised_prediction[\"Prediction 10\"]=[np.argmax(value) for value in model[9].predict(features[[*range(90,100)]])]\n",
    "    \n",
    "    result_dataset=features.copy()\n",
    "    \n",
    "    accepted_predictions=[]\n",
    "    \n",
    "    print(\"ENSEMBLING\")\n",
    "    for index in tqdm.tqdm(semi_supervised_prediction.index):\n",
    "        predictions=[semi_supervised_prediction[\"Prediction {}\".format(i)][index] for i in range(1,10+1)]\n",
    "        \n",
    "        if(len(set(predictions))==1):\n",
    "            accepted_predictions.append(semi_supervised_prediction[\"Prediction 1\"][index])\n",
    "        else:\n",
    "            accepted_predictions.append(-1)\n",
    "            \n",
    "    result_dataset[\"Labels\"]=accepted_predictions\n",
    "    \n",
    "    result_dataset=result_dataset.loc[result_dataset[\"Labels\"]!=-1]\n",
    "    \n",
    "    return result_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 300000/300000 [00:00<00:00, 1503534.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING PUNCTUATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 300000/300000 [00:05<00:00, 54293.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING SENTENCE TO VECTOR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 300000/300000 [00:24<00:00, 12305.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE VECTOR TO PANDAS DATAFRAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 1\n",
      "PREDICTION 2\n",
      "PREDICTION 3\n",
      "PREDICTION 4\n",
      "PREDICTION 5\n",
      "PREDICTION 6\n",
      "PREDICTION 7\n",
      "PREDICTION 8\n",
      "PREDICTION 9\n",
      "PREDICTION 10\n",
      "ENSEMBLING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 299848/299848 [00:47<00:00, 6268.48it/s]\n"
     ]
    }
   ],
   "source": [
    "additional_dataset_1 = semi_supervised_dataset(models,product_titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 300000/300000 [00:00<00:00, 1942538.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING PUNCTUATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 300000/300000 [00:05<00:00, 50810.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING SENTENCE TO VECTOR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 300000/300000 [00:26<00:00, 11325.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE VECTOR TO PANDAS DATAFRAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 1\n",
      "PREDICTION 2\n",
      "PREDICTION 3\n",
      "PREDICTION 4\n",
      "PREDICTION 5\n",
      "PREDICTION 6\n",
      "PREDICTION 7\n",
      "PREDICTION 8\n",
      "PREDICTION 9\n",
      "PREDICTION 10\n",
      "ENSEMBLING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 299971/299971 [00:48<00:00, 6203.36it/s]\n"
     ]
    }
   ],
   "source": [
    "additional_dataset_2 = semi_supervised_dataset(models,product_titles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 300000/300000 [00:00<00:00, 1719571.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING PUNCTUATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 300000/300000 [00:05<00:00, 53774.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING SENTENCE TO VECTOR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 300000/300000 [00:23<00:00, 12749.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE VECTOR TO PANDAS DATAFRAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  8.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 1\n",
      "PREDICTION 2\n",
      "PREDICTION 3\n",
      "PREDICTION 4\n",
      "PREDICTION 5\n",
      "PREDICTION 6\n",
      "PREDICTION 7\n",
      "PREDICTION 8\n",
      "PREDICTION 9\n",
      "PREDICTION 10\n",
      "ENSEMBLING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 299907/299907 [00:48<00:00, 6230.85it/s]\n"
     ]
    }
   ],
   "source": [
    "additional_dataset_3 = semi_supervised_dataset(models,product_titles[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 300000/300000 [00:00<00:00, 1634987.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING PUNCTUATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 300000/300000 [00:05<00:00, 58921.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING SENTENCE TO VECTOR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 300000/300000 [00:21<00:00, 13647.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE VECTOR TO PANDAS DATAFRAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 100/100 [00:11<00:00,  8.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 1\n",
      "PREDICTION 2\n",
      "PREDICTION 3\n",
      "PREDICTION 4\n",
      "PREDICTION 5\n",
      "PREDICTION 6\n",
      "PREDICTION 7\n",
      "PREDICTION 8\n",
      "PREDICTION 9\n",
      "PREDICTION 10\n",
      "ENSEMBLING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 299969/299969 [00:47<00:00, 6254.26it/s]\n"
     ]
    }
   ],
   "source": [
    "additional_dataset_4 = semi_supervised_dataset(models,product_titles[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 300000/300000 [00:00<00:00, 1286902.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING PUNCTUATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 300000/300000 [00:05<00:00, 55329.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING SENTENCE TO VECTOR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 300000/300000 [00:22<00:00, 13607.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE VECTOR TO PANDAS DATAFRAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 1\n",
      "PREDICTION 2\n",
      "PREDICTION 3\n",
      "PREDICTION 4\n",
      "PREDICTION 5\n",
      "PREDICTION 6\n",
      "PREDICTION 7\n",
      "PREDICTION 8\n",
      "PREDICTION 9\n",
      "PREDICTION 10\n",
      "ENSEMBLING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 299969/299969 [00:53<00:00, 5649.24it/s]\n"
     ]
    }
   ],
   "source": [
    "additional_dataset_5 = semi_supervised_dataset(models,product_titles[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 300000/300000 [00:00<00:00, 1201614.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING PUNCTUATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 300000/300000 [00:05<00:00, 56271.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING SENTENCE TO VECTOR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 300000/300000 [00:22<00:00, 13179.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE VECTOR TO PANDAS DATAFRAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 1\n",
      "PREDICTION 2\n",
      "PREDICTION 3\n",
      "PREDICTION 4\n",
      "PREDICTION 5\n",
      "PREDICTION 6\n",
      "PREDICTION 7\n",
      "PREDICTION 8\n",
      "PREDICTION 9\n",
      "PREDICTION 10\n",
      "ENSEMBLING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 299989/299989 [00:48<00:00, 6227.69it/s]\n"
     ]
    }
   ],
   "source": [
    "additional_dataset_6 = semi_supervised_dataset(models,product_titles[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 138532/138532 [00:00<00:00, 1222697.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REMOVING PUNCTUATIONS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 138532/138532 [00:02<00:00, 58543.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING SENTENCE TO VECTOR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 138532/138532 [00:10<00:00, 13178.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVE VECTOR TO PANDAS DATAFRAME\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION 1\n",
      "PREDICTION 2\n",
      "PREDICTION 3\n",
      "PREDICTION 4\n",
      "PREDICTION 5\n",
      "PREDICTION 6\n",
      "PREDICTION 7\n",
      "PREDICTION 8\n",
      "PREDICTION 9\n",
      "PREDICTION 10\n",
      "ENSEMBLING\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 138529/138529 [00:21<00:00, 6335.89it/s]\n"
     ]
    }
   ],
   "source": [
    "additional_dataset_7 = semi_supervised_dataset(models,product_titles[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_dataset=pd.concat([additional_dataset_1,additional_dataset_2,additional_dataset_3,additional_dataset_4,additional_dataset_5,additional_dataset_6,additional_dataset_7],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_dataset.to_csv(\"data/additional_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset=pd.concat([additional_data_set,large_embedded_data],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433207, 101)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=new_dataset.copy()\n",
    "sampled_embedded_data=data.sample(n=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_X_train,nn_X_test,nn_y_train,nn_y_test=train_test_split(sampled_embedded_data.drop(\"Labels\",axis=1),to_categorical(sampled_embedded_data[\"Labels\"]),test_size=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callback = MyCallback(nn_X_test, nn_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2000, input_shape=(100,), activation='relu'))\n",
    "model.add(Dense(1500, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(20, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20400/424542 [>.............................] - ETA: 2:07:19 - loss: 3.0351 - acc: 0.03 - ETA: 1:09:04 - loss: 2.8415 - acc: 0.11 - ETA: 50:08 - loss: 2.6761 - acc: 0.1767 - ETA: 40:15 - loss: 2.6169 - acc: 0.20 - ETA: 34:24 - loss: 2.5070 - acc: 0.25 - ETA: 30:24 - loss: 2.4223 - acc: 0.28 - ETA: 27:38 - loss: 2.3545 - acc: 0.31 - ETA: 25:33 - loss: 2.2549 - acc: 0.33 - ETA: 23:55 - loss: 2.2110 - acc: 0.34 - ETA: 22:45 - loss: 2.1750 - acc: 0.36 - ETA: 21:42 - loss: 2.0848 - acc: 0.38 - ETA: 20:53 - loss: 2.0599 - acc: 0.39 - ETA: 20:12 - loss: 2.0129 - acc: 0.40 - ETA: 19:32 - loss: 1.9778 - acc: 0.41 - ETA: 18:59 - loss: 1.9635 - acc: 0.42 - ETA: 18:33 - loss: 1.9370 - acc: 0.43 - ETA: 18:05 - loss: 1.9151 - acc: 0.43 - ETA: 17:40 - loss: 1.8926 - acc: 0.44 - ETA: 17:19 - loss: 1.8667 - acc: 0.45 - ETA: 16:59 - loss: 1.8439 - acc: 0.45 - ETA: 16:42 - loss: 1.8112 - acc: 0.46 - ETA: 16:32 - loss: 1.7874 - acc: 0.47 - ETA: 16:21 - loss: 1.7745 - acc: 0.47 - ETA: 16:12 - loss: 1.7574 - acc: 0.48 - ETA: 16:03 - loss: 1.7480 - acc: 0.48 - ETA: 15:54 - loss: 1.7396 - acc: 0.48 - ETA: 15:44 - loss: 1.7260 - acc: 0.49 - ETA: 15:34 - loss: 1.7066 - acc: 0.49 - ETA: 15:24 - loss: 1.6893 - acc: 0.50 - ETA: 15:17 - loss: 1.6670 - acc: 0.50 - ETA: 15:08 - loss: 1.6609 - acc: 0.51 - ETA: 15:01 - loss: 1.6520 - acc: 0.51 - ETA: 14:55 - loss: 1.6440 - acc: 0.51 - ETA: 14:50 - loss: 1.6326 - acc: 0.51 - ETA: 14:44 - loss: 1.6278 - acc: 0.52 - ETA: 14:39 - loss: 1.6196 - acc: 0.52 - ETA: 14:34 - loss: 1.6084 - acc: 0.52 - ETA: 14:30 - loss: 1.6000 - acc: 0.52 - ETA: 14:26 - loss: 1.5953 - acc: 0.52 - ETA: 14:22 - loss: 1.5875 - acc: 0.53 - ETA: 14:17 - loss: 1.5797 - acc: 0.53 - ETA: 14:13 - loss: 1.5795 - acc: 0.53 - ETA: 14:08 - loss: 1.5770 - acc: 0.53 - ETA: 14:04 - loss: 1.5613 - acc: 0.54 - ETA: 14:00 - loss: 1.5573 - acc: 0.54 - ETA: 13:59 - loss: 1.5533 - acc: 0.54 - ETA: 13:58 - loss: 1.5472 - acc: 0.54 - ETA: 13:57 - loss: 1.5393 - acc: 0.54 - ETA: 13:55 - loss: 1.5363 - acc: 0.55 - ETA: 13:51 - loss: 1.5276 - acc: 0.55 - ETA: 13:48 - loss: 1.5270 - acc: 0.55 - ETA: 13:45 - loss: 1.5218 - acc: 0.55 - ETA: 13:44 - loss: 1.5207 - acc: 0.55 - ETA: 13:42 - loss: 1.5148 - acc: 0.55 - ETA: 13:40 - loss: 1.5079 - acc: 0.55 - ETA: 13:37 - loss: 1.5047 - acc: 0.55 - ETA: 13:34 - loss: 1.4966 - acc: 0.56 - ETA: 13:32 - loss: 1.4921 - acc: 0.56 - ETA: 13:31 - loss: 1.4918 - acc: 0.56 - ETA: 13:29 - loss: 1.4894 - acc: 0.56 - ETA: 13:28 - loss: 1.4811 - acc: 0.56 - ETA: 13:28 - loss: 1.4815 - acc: 0.56 - ETA: 13:27 - loss: 1.4727 - acc: 0.56 - ETA: 13:25 - loss: 1.4699 - acc: 0.56 - ETA: 13:24 - loss: 1.4610 - acc: 0.57 - ETA: 13:22 - loss: 1.4566 - acc: 0.57 - ETA: 13:20 - loss: 1.4588 - acc: 0.57 - ETA: 13:19 - loss: 1.4578 - acc: 0.57 - ETA: 13:17 - loss: 1.4546 - acc: 0.57 - ETA: 13:16 - loss: 1.4552 - acc: 0.57 - ETA: 13:15 - loss: 1.4538 - acc: 0.57 - ETA: 13:15 - loss: 1.4502 - acc: 0.57 - ETA: 13:15 - loss: 1.4481 - acc: 0.57 - ETA: 13:13 - loss: 1.4464 - acc: 0.57 - ETA: 13:13 - loss: 1.4409 - acc: 0.57 - ETA: 13:12 - loss: 1.4383 - acc: 0.57 - ETA: 13:11 - loss: 1.4357 - acc: 0.57 - ETA: 13:10 - loss: 1.4324 - acc: 0.58 - ETA: 13:08 - loss: 1.4293 - acc: 0.58 - ETA: 13:06 - loss: 1.4277 - acc: 0.58 - ETA: 13:05 - loss: 1.4240 - acc: 0.58 - ETA: 13:04 - loss: 1.4191 - acc: 0.58 - ETA: 13:03 - loss: 1.4209 - acc: 0.58 - ETA: 13:02 - loss: 1.4189 - acc: 0.58 - ETA: 13:01 - loss: 1.4174 - acc: 0.58 - ETA: 13:00 - loss: 1.4110 - acc: 0.58 - ETA: 13:00 - loss: 1.4068 - acc: 0.58 - ETA: 13:00 - loss: 1.4036 - acc: 0.58 - ETA: 12:59 - loss: 1.4025 - acc: 0.58 - ETA: 12:59 - loss: 1.4002 - acc: 0.58 - ETA: 12:59 - loss: 1.3974 - acc: 0.58 - ETA: 12:58 - loss: 1.3936 - acc: 0.59 - ETA: 12:57 - loss: 1.3904 - acc: 0.59 - ETA: 12:56 - loss: 1.3940 - acc: 0.59 - ETA: 12:55 - loss: 1.3907 - acc: 0.59 - ETA: 12:54 - loss: 1.3885 - acc: 0.59 - ETA: 12:54 - loss: 1.3852 - acc: 0.59 - ETA: 12:53 - loss: 1.3811 - acc: 0.59 - ETA: 12:52 - loss: 1.3798 - acc: 0.59 - ETA: 12:51 - loss: 1.3764 - acc: 0.59 - ETA: 12:50 - loss: 1.3759 - acc: 0.59 - ETA: 12:50 - loss: 1.3728 - acc: 0.59 - ETA: 12:49 - loss: 1.3718 - acc: 0.59 - ETA: 12:48 - loss: 1.3708 - acc: 0.59 - ETA: 12:47 - loss: 1.3674 - acc: 0.59 - ETA: 12:47 - loss: 1.3648 - acc: 0.59 - ETA: 12:46 - loss: 1.3631 - acc: 0.59 - ETA: 12:47 - loss: 1.3610 - acc: 0.59 - ETA: 12:46 - loss: 1.3567 - acc: 0.60 - ETA: 12:45 - loss: 1.3556 - acc: 0.60 - ETA: 12:45 - loss: 1.3543 - acc: 0.60 - ETA: 12:45 - loss: 1.3543 - acc: 0.60 - ETA: 12:46 - loss: 1.3528 - acc: 0.60 - ETA: 12:46 - loss: 1.3505 - acc: 0.60 - ETA: 12:47 - loss: 1.3500 - acc: 0.60 - ETA: 12:47 - loss: 1.3495 - acc: 0.60 - ETA: 12:48 - loss: 1.3482 - acc: 0.60 - ETA: 12:48 - loss: 1.3469 - acc: 0.60 - ETA: 12:47 - loss: 1.3441 - acc: 0.60 - ETA: 12:46 - loss: 1.3423 - acc: 0.60 - ETA: 12:46 - loss: 1.3407 - acc: 0.60 - ETA: 12:46 - loss: 1.3380 - acc: 0.60 - ETA: 12:46 - loss: 1.3369 - acc: 0.60 - ETA: 12:45 - loss: 1.3378 - acc: 0.60 - ETA: 12:44 - loss: 1.3361 - acc: 0.60 - ETA: 12:43 - loss: 1.3357 - acc: 0.60 - ETA: 12:43 - loss: 1.3368 - acc: 0.60 - ETA: 12:42 - loss: 1.3349 - acc: 0.60 - ETA: 12:41 - loss: 1.3322 - acc: 0.60 - ETA: 12:41 - loss: 1.3295 - acc: 0.60 - ETA: 12:40 - loss: 1.3285 - acc: 0.60 - ETA: 12:40 - loss: 1.3289 - acc: 0.60 - ETA: 12:39 - loss: 1.3284 - acc: 0.60 - ETA: 12:40 - loss: 1.3268 - acc: 0.61 - ETA: 12:39 - loss: 1.3269 - acc: 0.61 - ETA: 12:40 - loss: 1.3245 - acc: 0.61 - ETA: 12:40 - loss: 1.3229 - acc: 0.61 - ETA: 12:41 - loss: 1.3227 - acc: 0.61 - ETA: 12:41 - loss: 1.3199 - acc: 0.61 - ETA: 12:42 - loss: 1.3184 - acc: 0.61 - ETA: 12:42 - loss: 1.3170 - acc: 0.61 - ETA: 12:42 - loss: 1.3137 - acc: 0.61 - ETA: 12:41 - loss: 1.3131 - acc: 0.61 - ETA: 12:41 - loss: 1.3105 - acc: 0.61 - ETA: 12:41 - loss: 1.3086 - acc: 0.61 - ETA: 12:40 - loss: 1.3057 - acc: 0.61 - ETA: 12:40 - loss: 1.3040 - acc: 0.61 - ETA: 12:40 - loss: 1.3029 - acc: 0.61 - ETA: 12:39 - loss: 1.3017 - acc: 0.61 - ETA: 12:39 - loss: 1.3008 - acc: 0.61 - ETA: 12:38 - loss: 1.3003 - acc: 0.61 - ETA: 12:38 - loss: 1.2980 - acc: 0.61 - ETA: 12:37 - loss: 1.2957 - acc: 0.61 - ETA: 12:37 - loss: 1.2943 - acc: 0.62 - ETA: 12:36 - loss: 1.2948 - acc: 0.62 - ETA: 12:36 - loss: 1.2926 - acc: 0.62 - ETA: 12:35 - loss: 1.2925 - acc: 0.62 - ETA: 12:35 - loss: 1.2910 - acc: 0.62 - ETA: 12:35 - loss: 1.2905 - acc: 0.62 - ETA: 12:34 - loss: 1.2905 - acc: 0.62 - ETA: 12:34 - loss: 1.2891 - acc: 0.62 - ETA: 12:33 - loss: 1.2885 - acc: 0.62 - ETA: 12:33 - loss: 1.2869 - acc: 0.62 - ETA: 12:32 - loss: 1.2865 - acc: 0.62 - ETA: 12:32 - loss: 1.2864 - acc: 0.62 - ETA: 12:32 - loss: 1.2848 - acc: 0.62 - ETA: 12:31 - loss: 1.2838 - acc: 0.62 - ETA: 12:31 - loss: 1.2826 - acc: 0.62 - ETA: 12:31 - loss: 1.2808 - acc: 0.62 - ETA: 12:30 - loss: 1.2783 - acc: 0.62 - ETA: 12:30 - loss: 1.2773 - acc: 0.62 - ETA: 12:29 - loss: 1.2777 - acc: 0.62 - ETA: 12:29 - loss: 1.2783 - acc: 0.62 - ETA: 12:29 - loss: 1.2763 - acc: 0.62 - ETA: 12:28 - loss: 1.2756 - acc: 0.62 - ETA: 12:28 - loss: 1.2749 - acc: 0.62 - ETA: 12:27 - loss: 1.2736 - acc: 0.62 - ETA: 12:27 - loss: 1.2742 - acc: 0.62 - ETA: 12:27 - loss: 1.2735 - acc: 0.62 - ETA: 12:26 - loss: 1.2727 - acc: 0.62 - ETA: 12:27 - loss: 1.2714 - acc: 0.62 - ETA: 12:27 - loss: 1.2692 - acc: 0.62 - ETA: 12:27 - loss: 1.2683 - acc: 0.62 - ETA: 12:26 - loss: 1.2668 - acc: 0.63 - ETA: 12:26 - loss: 1.2662 - acc: 0.63 - ETA: 12:26 - loss: 1.2651 - acc: 0.63 - ETA: 12:25 - loss: 1.2648 - acc: 0.63 - ETA: 12:25 - loss: 1.2658 - acc: 0.63 - ETA: 12:25 - loss: 1.2644 - acc: 0.63 - ETA: 12:25 - loss: 1.2637 - acc: 0.63 - ETA: 12:25 - loss: 1.2621 - acc: 0.63 - ETA: 12:24 - loss: 1.2606 - acc: 0.63 - ETA: 12:24 - loss: 1.2585 - acc: 0.63 - ETA: 12:24 - loss: 1.2584 - acc: 0.63 - ETA: 12:24 - loss: 1.2576 - acc: 0.63 - ETA: 12:24 - loss: 1.2575 - acc: 0.63 - ETA: 12:24 - loss: 1.2575 - acc: 0.63 - ETA: 12:23 - loss: 1.2571 - acc: 0.63 - ETA: 12:23 - loss: 1.2561 - acc: 0.63 - ETA: 12:23 - loss: 1.2549 - acc: 0.63 - ETA: 12:22 - loss: 1.2539 - acc: 0.63 - ETA: 12:22 - loss: 1.2537 - acc: 0.63 - ETA: 12:22 - loss: 1.2536 - acc: 0.63 - ETA: 12:21 - loss: 1.2529 - acc: 0.6341\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33700/424542 [=>............................] - ETA: 12:21 - loss: 1.2518 - acc: 0.63 - ETA: 12:20 - loss: 1.2506 - acc: 0.63 - ETA: 12:20 - loss: 1.2492 - acc: 0.63 - ETA: 12:20 - loss: 1.2492 - acc: 0.63 - ETA: 12:19 - loss: 1.2474 - acc: 0.63 - ETA: 12:18 - loss: 1.2463 - acc: 0.63 - ETA: 12:17 - loss: 1.2460 - acc: 0.63 - ETA: 12:16 - loss: 1.2453 - acc: 0.63 - ETA: 12:16 - loss: 1.2455 - acc: 0.63 - ETA: 12:15 - loss: 1.2443 - acc: 0.63 - ETA: 12:14 - loss: 1.2430 - acc: 0.63 - ETA: 12:14 - loss: 1.2422 - acc: 0.63 - ETA: 12:13 - loss: 1.2419 - acc: 0.63 - ETA: 12:13 - loss: 1.2412 - acc: 0.63 - ETA: 12:12 - loss: 1.2416 - acc: 0.63 - ETA: 12:11 - loss: 1.2411 - acc: 0.63 - ETA: 12:11 - loss: 1.2404 - acc: 0.63 - ETA: 12:10 - loss: 1.2411 - acc: 0.63 - ETA: 12:09 - loss: 1.2403 - acc: 0.63 - ETA: 12:09 - loss: 1.2404 - acc: 0.63 - ETA: 12:08 - loss: 1.2395 - acc: 0.63 - ETA: 12:07 - loss: 1.2399 - acc: 0.63 - ETA: 12:06 - loss: 1.2397 - acc: 0.63 - ETA: 12:06 - loss: 1.2389 - acc: 0.63 - ETA: 12:06 - loss: 1.2394 - acc: 0.63 - ETA: 12:06 - loss: 1.2393 - acc: 0.63 - ETA: 12:06 - loss: 1.2394 - acc: 0.63 - ETA: 12:06 - loss: 1.2397 - acc: 0.63 - ETA: 12:05 - loss: 1.2393 - acc: 0.63 - ETA: 12:05 - loss: 1.2385 - acc: 0.63 - ETA: 12:05 - loss: 1.2375 - acc: 0.63 - ETA: 12:04 - loss: 1.2367 - acc: 0.63 - ETA: 12:03 - loss: 1.2357 - acc: 0.63 - ETA: 12:03 - loss: 1.2346 - acc: 0.64 - ETA: 12:02 - loss: 1.2343 - acc: 0.64 - ETA: 12:02 - loss: 1.2332 - acc: 0.64 - ETA: 12:01 - loss: 1.2323 - acc: 0.64 - ETA: 12:00 - loss: 1.2316 - acc: 0.64 - ETA: 12:00 - loss: 1.2302 - acc: 0.64 - ETA: 11:59 - loss: 1.2299 - acc: 0.64 - ETA: 11:59 - loss: 1.2302 - acc: 0.64 - ETA: 11:58 - loss: 1.2289 - acc: 0.64 - ETA: 11:58 - loss: 1.2281 - acc: 0.64 - ETA: 11:57 - loss: 1.2280 - acc: 0.64 - ETA: 11:57 - loss: 1.2281 - acc: 0.64 - ETA: 11:56 - loss: 1.2283 - acc: 0.64 - ETA: 11:56 - loss: 1.2273 - acc: 0.64 - ETA: 11:55 - loss: 1.2259 - acc: 0.64 - ETA: 11:55 - loss: 1.2247 - acc: 0.64 - ETA: 11:54 - loss: 1.2240 - acc: 0.64 - ETA: 11:53 - loss: 1.2233 - acc: 0.64 - ETA: 11:53 - loss: 1.2223 - acc: 0.64 - ETA: 11:52 - loss: 1.2214 - acc: 0.64 - ETA: 11:52 - loss: 1.2220 - acc: 0.64 - ETA: 11:51 - loss: 1.2229 - acc: 0.64 - ETA: 11:51 - loss: 1.2224 - acc: 0.64 - ETA: 11:50 - loss: 1.2221 - acc: 0.64 - ETA: 11:50 - loss: 1.2217 - acc: 0.64 - ETA: 11:49 - loss: 1.2217 - acc: 0.64 - ETA: 11:49 - loss: 1.2206 - acc: 0.64 - ETA: 11:48 - loss: 1.2202 - acc: 0.64 - ETA: 11:48 - loss: 1.2185 - acc: 0.64 - ETA: 11:48 - loss: 1.2181 - acc: 0.64 - ETA: 11:47 - loss: 1.2173 - acc: 0.64 - ETA: 11:47 - loss: 1.2168 - acc: 0.64 - ETA: 11:47 - loss: 1.2159 - acc: 0.64 - ETA: 11:47 - loss: 1.2163 - acc: 0.64 - ETA: 11:46 - loss: 1.2147 - acc: 0.64 - ETA: 11:46 - loss: 1.2150 - acc: 0.64 - ETA: 11:45 - loss: 1.2146 - acc: 0.64 - ETA: 11:45 - loss: 1.2137 - acc: 0.64 - ETA: 11:44 - loss: 1.2132 - acc: 0.64 - ETA: 11:44 - loss: 1.2130 - acc: 0.64 - ETA: 11:43 - loss: 1.2127 - acc: 0.64 - ETA: 11:43 - loss: 1.2119 - acc: 0.64 - ETA: 11:42 - loss: 1.2114 - acc: 0.64 - ETA: 11:42 - loss: 1.2101 - acc: 0.64 - ETA: 11:41 - loss: 1.2094 - acc: 0.64 - ETA: 11:41 - loss: 1.2087 - acc: 0.64 - ETA: 11:41 - loss: 1.2075 - acc: 0.64 - ETA: 11:40 - loss: 1.2070 - acc: 0.64 - ETA: 11:40 - loss: 1.2068 - acc: 0.64 - ETA: 11:40 - loss: 1.2072 - acc: 0.64 - ETA: 11:40 - loss: 1.2059 - acc: 0.64 - ETA: 11:39 - loss: 1.2051 - acc: 0.64 - ETA: 11:39 - loss: 1.2050 - acc: 0.64 - ETA: 11:39 - loss: 1.2042 - acc: 0.64 - ETA: 11:39 - loss: 1.2038 - acc: 0.64 - ETA: 11:38 - loss: 1.2024 - acc: 0.64 - ETA: 11:38 - loss: 1.2017 - acc: 0.65 - ETA: 11:37 - loss: 1.2008 - acc: 0.65 - ETA: 11:37 - loss: 1.2006 - acc: 0.65 - ETA: 11:36 - loss: 1.2005 - acc: 0.65 - ETA: 11:36 - loss: 1.2002 - acc: 0.65 - ETA: 11:36 - loss: 1.1997 - acc: 0.65 - ETA: 11:35 - loss: 1.1985 - acc: 0.65 - ETA: 11:35 - loss: 1.1973 - acc: 0.65 - ETA: 11:34 - loss: 1.1972 - acc: 0.65 - ETA: 11:34 - loss: 1.1973 - acc: 0.65 - ETA: 11:33 - loss: 1.1967 - acc: 0.65 - ETA: 11:33 - loss: 1.1964 - acc: 0.65 - ETA: 11:32 - loss: 1.1959 - acc: 0.65 - ETA: 11:32 - loss: 1.1951 - acc: 0.65 - ETA: 11:31 - loss: 1.1946 - acc: 0.65 - ETA: 11:31 - loss: 1.1944 - acc: 0.65 - ETA: 11:31 - loss: 1.1947 - acc: 0.65 - ETA: 11:31 - loss: 1.1941 - acc: 0.65 - ETA: 11:31 - loss: 1.1939 - acc: 0.65 - ETA: 11:30 - loss: 1.1931 - acc: 0.65 - ETA: 11:30 - loss: 1.1925 - acc: 0.65 - ETA: 11:30 - loss: 1.1926 - acc: 0.65 - ETA: 11:31 - loss: 1.1925 - acc: 0.65 - ETA: 11:30 - loss: 1.1926 - acc: 0.65 - ETA: 11:30 - loss: 1.1923 - acc: 0.65 - ETA: 11:30 - loss: 1.1912 - acc: 0.65 - ETA: 11:30 - loss: 1.1905 - acc: 0.65 - ETA: 11:30 - loss: 1.1889 - acc: 0.65 - ETA: 11:30 - loss: 1.1891 - acc: 0.65 - ETA: 11:30 - loss: 1.1892 - acc: 0.65 - ETA: 11:30 - loss: 1.1887 - acc: 0.65 - ETA: 11:30 - loss: 1.1883 - acc: 0.65 - ETA: 11:29 - loss: 1.1876 - acc: 0.65 - ETA: 11:29 - loss: 1.1870 - acc: 0.65 - ETA: 11:29 - loss: 1.1862 - acc: 0.65 - ETA: 11:29 - loss: 1.1856 - acc: 0.65 - ETA: 11:29 - loss: 1.1854 - acc: 0.65 - ETA: 11:29 - loss: 1.1848 - acc: 0.65 - ETA: 11:29 - loss: 1.1846 - acc: 0.65 - ETA: 11:29 - loss: 1.1844 - acc: 0.65 - ETA: 11:28 - loss: 1.1836 - acc: 0.65 - ETA: 11:28 - loss: 1.1836 - acc: 0.65 - ETA: 11:28 - loss: 1.1839 - acc: 0.65 - ETA: 11:28 - loss: 1.1830 - acc: 0.6559"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-bb3e8f02e589>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn_X_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1705\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1706\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1236\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(nn_X_train, nn_y_train, epochs=5, batch_size=100, shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
