{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Useful Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "D:\\Program\\Anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "D:\\Program\\Anaconda\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "D:\\Program\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import gmtime, strftime\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Embedder\n",
    "from gensim.models import FastText\n",
    "\n",
    "# Classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.grid_search import GridSearchCV as GS\n",
    "from sklearn.model_selection import validation_curve, learning_curve\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures as Poly\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine Model's File Location\n",
    "\n",
    "version = \"version_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_pipeline import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/{}/label_encoder.pickle\".format(version), \"rb\") as file:\n",
    "    label_encoder = pickle.load(file)\n",
    "\n",
    "with open(\"model/{}/word_embedder.pickle\".format(version), \"rb\") as file:\n",
    "    word_embedder = pickle.load(file)\n",
    "\n",
    "neural_network = load_model(\"model/{}/neural_network.h5\".format(version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Category Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor=preprocessing(word_embedder.vector_size,word_embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.fasttext.FastText at 0x4e9d2c77f0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x4e9f6e2ef0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data To Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_data_for_classification=pd.read_csv(\"data/big.csv\",header=None)\n",
    "large_data_for_classification.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hardware</td>\n",
       "      <td>KINGSTON+KVR1333D3N9</td>\n",
       "      <td>1510.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>musik</td>\n",
       "      <td>power+amplifier+wisdom+</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>outwear-motor</td>\n",
       "      <td>jas%20hujan%20anak</td>\n",
       "      <td>391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>celana</td>\n",
       "      <td>Celana+bahan+formal</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>komputer</td>\n",
       "      <td>Preset+lightroom</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0                        1       2\n",
       "0       hardware     KINGSTON+KVR1333D3N9  1510.0\n",
       "1          musik  power+amplifier+wisdom+    62.0\n",
       "2  outwear-motor       jas%20hujan%20anak   391.0\n",
       "3         celana      Celana+bahan+formal   288.0\n",
       "4       komputer         Preset+lightroom     1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_data_for_classification.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess product title to 100-dimensional vector\n",
    "#and preprocess category name to integer label\n",
    "large_embedded_data = preprocessor.preprocess_data(\n",
    "    large_data_for_classification[1],\n",
    "    large_data_for_classification[0],\n",
    "    label_encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.580157</td>\n",
       "      <td>-0.902683</td>\n",
       "      <td>-0.200629</td>\n",
       "      <td>0.426924</td>\n",
       "      <td>-0.298552</td>\n",
       "      <td>0.377382</td>\n",
       "      <td>0.334578</td>\n",
       "      <td>-1.341522</td>\n",
       "      <td>0.284654</td>\n",
       "      <td>-0.601798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714149</td>\n",
       "      <td>0.436149</td>\n",
       "      <td>0.067786</td>\n",
       "      <td>0.280465</td>\n",
       "      <td>-0.069688</td>\n",
       "      <td>0.040827</td>\n",
       "      <td>0.063288</td>\n",
       "      <td>0.042661</td>\n",
       "      <td>-0.402801</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.151489</td>\n",
       "      <td>-1.193317</td>\n",
       "      <td>-0.412542</td>\n",
       "      <td>0.074164</td>\n",
       "      <td>-1.053800</td>\n",
       "      <td>0.283322</td>\n",
       "      <td>0.306509</td>\n",
       "      <td>-0.714775</td>\n",
       "      <td>-0.282685</td>\n",
       "      <td>-0.205421</td>\n",
       "      <td>...</td>\n",
       "      <td>1.560397</td>\n",
       "      <td>1.276572</td>\n",
       "      <td>-0.248248</td>\n",
       "      <td>0.154423</td>\n",
       "      <td>-0.181553</td>\n",
       "      <td>-0.448146</td>\n",
       "      <td>-0.126884</td>\n",
       "      <td>-0.248857</td>\n",
       "      <td>1.153092</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.206052</td>\n",
       "      <td>-0.158160</td>\n",
       "      <td>-0.194283</td>\n",
       "      <td>0.238795</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>0.196439</td>\n",
       "      <td>0.141539</td>\n",
       "      <td>-0.223540</td>\n",
       "      <td>0.046263</td>\n",
       "      <td>-0.282032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081149</td>\n",
       "      <td>0.099055</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>0.099320</td>\n",
       "      <td>-0.081225</td>\n",
       "      <td>0.031153</td>\n",
       "      <td>-0.080204</td>\n",
       "      <td>-0.028578</td>\n",
       "      <td>-0.037810</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.053166</td>\n",
       "      <td>0.007892</td>\n",
       "      <td>-1.529572</td>\n",
       "      <td>1.458670</td>\n",
       "      <td>-0.734888</td>\n",
       "      <td>-0.503057</td>\n",
       "      <td>-0.532288</td>\n",
       "      <td>-0.022648</td>\n",
       "      <td>0.070379</td>\n",
       "      <td>-1.529616</td>\n",
       "      <td>...</td>\n",
       "      <td>1.294445</td>\n",
       "      <td>-0.273994</td>\n",
       "      <td>1.718030</td>\n",
       "      <td>0.060490</td>\n",
       "      <td>-1.941472</td>\n",
       "      <td>1.189055</td>\n",
       "      <td>-0.650072</td>\n",
       "      <td>-0.596036</td>\n",
       "      <td>0.285584</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.162186</td>\n",
       "      <td>-0.530731</td>\n",
       "      <td>-0.356022</td>\n",
       "      <td>0.209101</td>\n",
       "      <td>-0.282881</td>\n",
       "      <td>0.324258</td>\n",
       "      <td>0.422291</td>\n",
       "      <td>-0.745394</td>\n",
       "      <td>-0.038498</td>\n",
       "      <td>-0.477472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860707</td>\n",
       "      <td>0.729105</td>\n",
       "      <td>-0.046928</td>\n",
       "      <td>0.239313</td>\n",
       "      <td>0.133619</td>\n",
       "      <td>-0.398247</td>\n",
       "      <td>0.280034</td>\n",
       "      <td>0.195425</td>\n",
       "      <td>0.424736</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.580157 -0.902683 -0.200629  0.426924 -0.298552  0.377382  0.334578   \n",
       "1 -0.151489 -1.193317 -0.412542  0.074164 -1.053800  0.283322  0.306509   \n",
       "2 -0.206052 -0.158160 -0.194283  0.238795  0.007292  0.196439  0.141539   \n",
       "3 -0.053166  0.007892 -1.529572  1.458670 -0.734888 -0.503057 -0.532288   \n",
       "4 -0.162186 -0.530731 -0.356022  0.209101 -0.282881  0.324258  0.422291   \n",
       "\n",
       "          7         8         9   ...          91        92        93  \\\n",
       "0 -1.341522  0.284654 -0.601798   ...    0.714149  0.436149  0.067786   \n",
       "1 -0.714775 -0.282685 -0.205421   ...    1.560397  1.276572 -0.248248   \n",
       "2 -0.223540  0.046263 -0.282032   ...   -0.081149  0.099055  0.030815   \n",
       "3 -0.022648  0.070379 -1.529616   ...    1.294445 -0.273994  1.718030   \n",
       "4 -0.745394 -0.038498 -0.477472   ...    0.860707  0.729105 -0.046928   \n",
       "\n",
       "         94        95        96        97        98        99  Labels  \n",
       "0  0.280465 -0.069688  0.040827  0.063288  0.042661 -0.402801      45  \n",
       "1  0.154423 -0.181553 -0.448146 -0.126884 -0.248857  1.153092      68  \n",
       "2  0.099320 -0.081225  0.031153 -0.080204 -0.028578 -0.037810      73  \n",
       "3  0.060490 -1.941472  1.189055 -0.650072 -0.596036  0.285584      24  \n",
       "4  0.239313  0.133619 -0.398247  0.280034  0.195425  0.424736      59  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_embedded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396099, 101)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_embedded_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_product_class(query,preprocessor=preprocessor,classifier=neural_network,label_encoder=label_encoder):\n",
    "    no_parentheses=preprocessor.remove_parentheses(query)\n",
    "    embedded_query=preprocessor.vectorize_sentence(no_parentheses).reshape(-1,100)\n",
    "    prediction=classifier.predict_classes(embedded_query)\n",
    "    class_prediction=label_encoder.inverse_transform(prediction[0])\n",
    "    return class_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted,truth):\n",
    "    result=[int(value) for value in np.array(predicted)==np.array(truth)]\n",
    "    return sum(result)/len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=neural_network\n",
    "data=large_embedded_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_embedded_data=data.sample(n=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.8037409839459327\n"
     ]
    }
   ],
   "source": [
    "truth=data[\"Labels\"]\n",
    "pred=model.predict_classes(data.drop(\"Labels\",axis=1))\n",
    "print(\"Accuracy : {}\".format(accuracy(pred,truth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.81      0.78       997\n",
      "          1       0.80      0.80      0.80     11550\n",
      "          2       0.88      0.77      0.82     16564\n",
      "          3       0.67      0.49      0.57      1147\n",
      "          4       0.42      0.29      0.34        17\n",
      "          5       0.75      0.80      0.77      9782\n",
      "          6       0.76      0.77      0.76     13987\n",
      "          7       0.70      0.64      0.67      1323\n",
      "          8       0.72      0.67      0.69      1107\n",
      "          9       0.76      0.75      0.76      1717\n",
      "         10       0.65      0.58      0.61      1010\n",
      "         11       0.77      0.74      0.75      1515\n",
      "         12       0.53      0.57      0.55      1148\n",
      "         13       0.83      0.78      0.81      1880\n",
      "         14       0.85      0.92      0.89      1441\n",
      "         15       0.74      0.79      0.76       882\n",
      "         16       0.81      0.72      0.76      2046\n",
      "         17       0.81      0.86      0.84       942\n",
      "         18       0.93      0.93      0.93      3489\n",
      "         19       0.85      0.80      0.82      2864\n",
      "         20       0.77      0.78      0.78       306\n",
      "         21       0.92      0.88      0.90      5060\n",
      "         22       0.75      0.70      0.72       403\n",
      "         23       0.67      0.43      0.53       326\n",
      "         24       0.81      0.71      0.76       587\n",
      "         25       0.77      0.84      0.80      2202\n",
      "         26       0.49      0.32      0.39       298\n",
      "         27       0.73      0.75      0.74      3498\n",
      "         28       0.87      0.87      0.87      1220\n",
      "         29       0.71      0.56      0.62      1514\n",
      "         30       0.66      0.75      0.70     19578\n",
      "         31       0.70      0.75      0.72      4939\n",
      "         32       0.74      0.66      0.70      1457\n",
      "         33       0.70      0.62      0.66      1669\n",
      "         34       0.76      0.59      0.67       150\n",
      "         35       0.84      0.90      0.87     28262\n",
      "         36       0.83      0.76      0.79     13145\n",
      "         37       0.81      0.82      0.82      3405\n",
      "         38       0.86      0.88      0.87       621\n",
      "         39       0.78      0.75      0.77      1654\n",
      "         40       0.77      0.88      0.82      3396\n",
      "         41       0.44      0.37      0.40        30\n",
      "         42       0.77      0.77      0.77      6005\n",
      "         43       0.89      0.85      0.87       269\n",
      "         44       0.88      0.93      0.91     36257\n",
      "         45       0.84      0.84      0.84      4911\n",
      "         46       0.48      0.39      0.43       400\n",
      "         47       0.64      0.58      0.61       931\n",
      "         48       0.62      0.56      0.59      4321\n",
      "         49       0.71      0.56      0.63       573\n",
      "         50       0.85      0.95      0.90      1200\n",
      "         51       0.73      0.68      0.70       929\n",
      "         52       0.87      0.89      0.88     10087\n",
      "         53       0.61      0.53      0.57      1156\n",
      "         54       0.62      0.53      0.57        19\n",
      "         55       0.58      0.77      0.66        84\n",
      "         56       0.83      0.53      0.65        19\n",
      "         57       0.82      0.90      0.86        20\n",
      "         58       0.78      0.77      0.77     10142\n",
      "         59       0.89      0.86      0.87      9250\n",
      "         60       0.68      0.43      0.53      1335\n",
      "         61       0.85      0.85      0.85     12677\n",
      "         62       0.77      0.83      0.80       198\n",
      "         63       0.83      0.74      0.78      2030\n",
      "         64       0.63      0.67      0.65      2576\n",
      "         65       0.71      0.63      0.67      1356\n",
      "         66       1.00      0.25      0.40         4\n",
      "         67       1.00      0.20      0.33         5\n",
      "         68       0.86      0.87      0.87      6742\n",
      "         69       0.82      0.79      0.80      3850\n",
      "         70       0.59      0.47      0.52      3655\n",
      "         71       0.74      0.65      0.69      6000\n",
      "         72       0.79      0.76      0.77      1881\n",
      "         73       0.87      0.87      0.87      8188\n",
      "         74       0.75      0.64      0.69       680\n",
      "         75       0.92      0.93      0.93      4614\n",
      "         76       0.90      0.85      0.88       605\n",
      "         77       0.61      0.83      0.70       200\n",
      "         78       0.68      0.64      0.66      1583\n",
      "         79       0.69      0.62      0.65      1230\n",
      "         80       0.72      0.77      0.74      1179\n",
      "         81       0.84      0.80      0.82      2910\n",
      "         82       0.72      0.56      0.63        78\n",
      "         83       0.55      0.50      0.52        22\n",
      "         84       0.84      0.86      0.85      3296\n",
      "         85       0.83      0.85      0.84      1529\n",
      "         86       0.55      0.46      0.50        26\n",
      "         87       0.67      0.69      0.68      1006\n",
      "         88       0.59      0.54      0.56       434\n",
      "         89       0.73      0.85      0.79       452\n",
      "         90       0.73      0.67      0.69     14690\n",
      "         91       0.88      0.82      0.85       377\n",
      "         92       0.74      0.50      0.60       554\n",
      "         93       0.80      0.81      0.81       603\n",
      "         94       0.77      0.73      0.75       335\n",
      "         95       0.91      0.90      0.90      5304\n",
      "         96       0.82      0.69      0.75      3076\n",
      "         97       0.76      0.89      0.82      9856\n",
      "         98       0.51      0.42      0.46       406\n",
      "         99       0.79      0.73      0.76        88\n",
      "        100       0.86      0.85      0.85     20812\n",
      "        101       0.83      0.77      0.80       791\n",
      "        102       0.75      0.60      0.67       131\n",
      "        103       0.77      0.86      0.81      5333\n",
      "        104       0.84      0.84      0.84      3506\n",
      "        105       0.86      0.71      0.78       249\n",
      "        106       0.79      0.88      0.83       327\n",
      "        107       0.73      0.78      0.76       372\n",
      "        108       0.84      0.84      0.84      2122\n",
      "        109       0.84      0.83      0.83      1125\n",
      "\n",
      "avg / total       0.80      0.80      0.80    396099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(truth,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
